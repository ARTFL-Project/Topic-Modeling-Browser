###############################################
## TOPOLOGIC CONFIGURATION FILE ##
###############################################

[SOURCE_DATA]
# Path to source datatabase, e.g. /var/www/html/philologic/ecco_tcp/
philologic_database_path =

# Link to Philologic database; e.g. http://my-server.com/philologic/ecco_tcp/
philologic_database_url =

[DATABASE]
# Name of database. Must be without spaces. Use underscores to separate words (no hyphens)
database_name =

[PREPROCESSING]
# Defines what object level to divide each text into
# Only posssible with a Philologic4 index
# Useful to break up a single document into smaller text units
text_object_level = doc

# Same as above but used only for training the model. You may want to train on smaller or bigger text objects
# depending on your corpus. If left blank, will default to the text_object_level value.
training_text_object_level =

# Language: set the language for various normalization tasks
# such as stemming, lemmatizing, word mapping...etc
language =

# Modernize language if modernization is available for your language: currently only French is supported.
modernize = yes

# Transliterate characters to closest ascii representation.
ascii = no

# Stem words using the Porter Stemmer
stemmer = yes

# Lemmatizer: path to lemmatizer file where each line contains the inflected form and
# the corresponding lemma separated by a tab
lemmatizer =

# Lowercase words
lowercase = yes

# Remove numbers
numbers = yes

# Minimum word length
minimum_word_length = 2

# Stopwords: path to stopword list
stopwords =

# Parts-of-speech to keep: specify which parts of speach to keep. Use notation from Spacy, see https://spacio.io
# Separate each pos to keep by a comma
pos_to_keep =

[VECTORIZATION]
# Vectorization type: choice between tf (Term Frequency), and tfidf (Term Frequency - Inverse Document Frequency)
vectorization = tfidf

# Minimum frequency of token: expressed as a floating number between 0 and 1
min_freq = 0.1

# Maximum frequency of token: expressed as a floating number between 0 and 1
max_freq = 0.9

# Max features: build a vocabulary that only consider the top max_features ordered by term frequency across the corpus. Ff empty, all vocab is included.
max_features =

# Defines how many tokens constitute a ngram
ngram = 3

# Minimum tokens per doc for training. If 0 is set, training data will include the entire corpus. If set to above 0, the filtered docs will be used as
# tranining data to predict topics on the whole corpus.
min_tokens_per_doc = 0


[TOPIC_MODELING]
# Algorithm for Topic Modeling: choice between lda (Latent Dirichlet Allocation),
# and nmf (Non-Negative Matrix Factorization)
algorithm = nmf

# Number of topics for model
number_of_topics = 100

# Maximum iteration for LDA model
max_iter = 20


[TOPICS_OVER_TIME]
# Define interval to calculate evolution of topic over time
topics_over_time_interval = 1

# Define start and end date. If not set, topologic will use the lowest and highest values in the collection
start_date =
end_date =