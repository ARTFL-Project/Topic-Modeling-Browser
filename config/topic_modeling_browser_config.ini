###############################################
## TOPIC MODELING BROWSER CONFIGURATION FILE ##
###############################################


[DATABASE]
# Name of database. Must be without spaces. Use underscores to separate words (no hyphens)
database_name =

[PREPROCESSING]
# Defines what object level to divide each text into
# Only posssible with a Philologic4 index
# Useful to break up a single document into smaller text units
text_object_level = doc

# Language: set the language for various normalization tasks
# such as stemming, lemmatizing, word mapping...etc
language =

# Modernize language if modernization is available for your language: currently only French is supported.
modernize = yes

# Transliterate characters to closest ascii representation.
ascii = no

# Stem words using the Porter Stemmer
stemmer = yes

# Lemmatizer: path to lemmatizer file where each line contains the inflected form and
# the corresponding lemma separated by a tab
lemmatizer =

# Lowercase words
lowercase = yes

# Remove numbers
numbers = yes

# Minimum word length
minimum_word_length = 2

# Stopwords: path to stopword list
stopwords =

# Parts-of-speech to keep: specify which parts of speach to keep. Use notation from Spacy, see https://spacio.io
# Separate each pos to keep by a comma
pos_to_keep =

[VECTORIZATION]
# Vectorization type: choice between tf (Term Frequency), and tfidf (Term Frequency - Inverse Document Frequency)
vectorization = tf

# Minimum frequency of token: expressed as a floating number between 0 and 1
min_freq = 0.1

# Maximum frequency of token: expressed as a floating number between 0 and 1
max_freq = 0.9

# Max features: build a vocabulary that only consider the top max_features ordered by term frequency across the corpus. Ff empty, all vocab is included.
max_features =

# Defines how many tokens constitute a ngram
ngram = 3

# Minimum tokens per doc for training. If 0 is set, training data will include the entire corpus. If set to above 0, the filtered docs will be used as
# tranining data to predict topics on the whole corpus.
min_tokens_per_doc = 0


[TOPIC_MODELING]
# Algorithm for Topic Modeling: choice between lda (Latent Dirichlet Allocation),
# and nmf (Non-Negative Matrix Factorization)
algorithm = lda

# Number of topics for model
number_of_topics = 100

# Maximum iteration for LDA model
max_iter = 20

