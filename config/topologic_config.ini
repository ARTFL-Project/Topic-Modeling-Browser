###############################################
## TOPOLOGIC CONFIGURATION FILE ##
###############################################

[TRAINING_DATA]
# Path to source datatabase, e.g. /var/www/html/philologic/ecco_tcp/. If several, use a comma distinguish each
philologic_database_paths =

# Link to Philologic database; e.g. http://my-server.com/philologic/ecco_tcp/. If several, use a comma to distinguish each. Must be in same order as the path.
philologic_database_urls =

# Defines what object level to divide each text into. Useful to break up a single document into smaller text units. If several, use a comma to distinguish each. Must be in same order as the path
text_object_level =

# Minimum tokens per doc for training. If 0 is set, training data will include the entire corpus.
min_tokens_per_doc = 0


[INFERENCE_DATA]
# Path to datatabases which will be classified, e.g. /var/www/html/philologic/ecco_tcp/. If several, use a comma distinguish each
philologic_database_paths =

# Link to Philologic database; e.g. http://my-server.com/philologic/ecco_tcp/. If several, use a comma to distinguish each. Must be in same order as the path.
philologic_database_urls =

# Defines what object level to divide each text into. Useful to break up a single document into smaller text units. If several, use a comma to distinguish each. Must be in same order as the path
text_object_level =

# Minimum tokens per doc for inference. If 0 is set, the model will be applied to the entire corpus. If set to above 0, only documents above the threshold will be classified and stored in the databse.
min_tokens_per_doc = 0


[METADATA_FILTERS]
### List one metadata field and one value per line, e.g. author = Rousseau
### The value for the query must use the PhiloLogic metadadata query syntax
### So egrep-style regexes as well as ranges are admissible, as well as NOT or OR queries


[DATABASE]
# Name of database. Must be without spaces. Use underscores to separate words (no hyphens)
database_name =

[PREPROCESSING]

# Language: set the language for various normalization tasks
# such as stemming, lemmatizing, word mapping...etc
language =

# Modernize language if modernization is available for your language: currently only French is supported.
modernize = yes

# Transliterate characters to closest ascii representation.
ascii = no

# Stem words using the Porter Stemmer
stemmer = yes

# Lemmatizer: path to lemmatizer file where each line contains the inflected form and
# the corresponding lemma separated by a tab
lemmatizer =

# Lowercase words
lowercase = yes

# Remove numbers
numbers = yes

# Minimum word length
minimum_word_length = 2

# Stopwords: path to stopword list
stopwords =

# Parts-of-speech to keep: specify which parts of speach to keep. Use notation from Spacy, see https://spacio.io
# Separate each pos to keep by a comma
pos_to_keep =

[VECTORIZATION]
# Vectorization type: choice between tf (Term Frequency), and tfidf (Term Frequency - Inverse Document Frequency)
vectorization = tfidf

# Minimum frequency of token: expressed as a floating number between 0 and 1
min_freq = 0.1

# Maximum frequency of token: expressed as a floating number between 0 and 1
max_freq = 0.9

# Max features: build a vocabulary that only consider the top max_features ordered by term frequency across the corpus. Ff empty, all vocab is included.
max_features =

# Defines how many tokens constitute a ngram
ngram = 1




[TOPIC_MODELING]
# Algorithm for Topic Modeling: choice between lda (Latent Dirichlet Allocation),
# and nmf (Non-Negative Matrix Factorization)
algorithm = nmf

# Number of topics for model
number_of_topics = 100

# Maximum iteration for model
max_iter = 200


[TOPICS_OVER_TIME]
# Define interval to calculate evolution of topic over time
topics_over_time_interval = 1

# Define start and end date. If not set, topologic will use the lowest and highest values in the collection
start_date =
end_date =