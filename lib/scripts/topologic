#!/usr/bin/env python3

import argparse
import configparser
import json
import os

from text_preprocessing import PreProcessor
from topologic import (
    read_config,
    write_app_config,
    year_normalizer,
    max_year_normalizer,
)
from topologic.corpus import Corpus
from topologic.DB import DBHandler
from topologic.topic_model import (
    LatentDirichletAllocation,
    NonNegativeMatrixFactorization,
)
from philologic.runtime.DB import DB
from topologic import topic_num_evaluator
from joblib import dump


GLOBAL_CONFIG = configparser.ConfigParser()
GLOBAL_CONFIG.read("/etc/topologic/global_settings.ini")


def parse_args():
    parser = argparse.ArgumentParser(description="Define files to process")
    parser.add_argument("--config", help="Configuration file", default="", type=str)
    parser.add_argument(
        "--data_output",
        help="path to local data to be saved during processing",
        default="./temp_preprocessed_data",
        type=str,
    )
    parser.add_argument(
        "--workers", help="How many threads or cores to use for preprocessing and modeling", type=int, default=4,
    )
    parser.add_argument(
        "--evaluate",
        help="Evaluate topic model. No topic models or web app will be saved",
        action="store_true",
        default=False,
    )
    parser.add_argument(
        "--min_num_topics", help="minimum number of topics for evaluation", type=int, default=10,
    )
    parser.add_argument(
        "--max_num_topics", help="maximum number of topics for evaluation", type=int, default=20,
    )
    parser.add_argument(
        "--debug", help="debug mode: temp file in /tmp will not be deleted.", action="store_true", default=False,
    )
    args = parser.parse_args()
    return args


def main(args):
    if args.config == "":
        print("No configuration file provided, exiting...")
        exit()
    (
        training_data,
        inference_data,
        metadata_filters,
        database_name,
        prep_config,
        vector_config,
        model_config,
        topics_over_time,
    ) = read_config(args.config)
    if os.path.exists(args.data_output) is True:
        os.system(f"rm -rf {args.data_output}")
    training_texts_path = os.path.join(args.data_output, "training/")
    inference_texts_path = os.path.join(args.data_output, "inference/")
    os.system(f"mkdir -p {inference_texts_path}")
    os.system(f"mkdir -p {training_texts_path}")

    print("## PREPROCESSING ##", flush=True)
    prepare_data(
        prep_config, training_data, training_texts_path, inference_data, inference_texts_path, metadata_filters,
    )

    topic_model, full_corpus, training_corpus = build_model(
        training_texts_path,
        inference_texts_path,
        training_data,
        inference_data,
        algorithm=model_config["algorithm"],
        number_of_topics=model_config["number_of_topics"],
        vectorization=vector_config["vectorization"],
        max_freq=vector_config["max_freq"],
        min_freq=vector_config["min_freq"],
        max_features=vector_config["max_features"] or None,
        ngram=vector_config["ngram"],
        min_tokens_per_doc=vector_config["min_tokens_per_doc"],
        evaluate=args.evaluate,
    )

    if args.evaluate is False:
        source_file_path = os.path.join(source_data["philologic_database_path"], "data/TEXT")
        build_web_app(
            args.config,
            database_name,
            metadata,
            topic_model,
            full_corpus,
            source_file_path,
            source_data["philologic_database_url"],
            prep_config["text_object_level"],
            topics_over_time,
            algorithm=model_config["algorithm"],
            number_of_topics=model_config["number_of_topics"],
            vectorization=vector_config["vectorization"],
            max_freq=vector_config["max_freq"],
            min_freq=vector_config["min_freq"],
            max_features=vector_config["max_features"] or None,
            ngram=vector_config["ngram"],
            min_tokens_per_doc=vector_config["min_tokens_per_doc"],
        )
    else:
        print("Estimating the number of topics...")
        corpus_path = os.path.join(args.data_output, "corpus")
        dump(training_corpus, corpus_path)
        os.system("mkdir -p ./evaluation_output")
        topic_num_evaluator(
            corpus_path,
            args.min_num_topics,
            args.max_num_topics,
            model_config["algorithm"],
            iterations=10,
            step=1,
            top_n_words=10,
            workers=args.workers,
        )

    if args.debug is False:
        os.system(f"rm -rf {args.data_output}")


def get_file_list(data_path, metadata_filters):
    philo_db = DB(data_path)
    hits = philo_db.query(
        qs="", method="", method_arg="", limit="", sort_order=["rowid"], raw_results=True, **metadata_filters,
    )
    hits.finish()
    philo_ids = {" ".join(str(i) for i in hit) for hit in hits}
    file_list = {os.path.join(data_path, f"words_and_philo_ids/{hit.split()[0]}") for hit in philo_ids}
    return file_list, philo_ids


def prepare_data(
    prep_config, training_data, training_texts_path, inference_data, inference_texts_path, metadata_filters,
):
    print("Processing training data...", flush=True)
    count = 0
    pos = 0
    for db_name, db_data in training_data.items():
        count += 1
        preproc = PreProcessor(
            text_object_type=db_data["text_object_level"],
            language=prep_config["language"],
            stemmer=prep_config["stemmer"],
            lemmatizer=prep_config["lemmatizer"],
            modernize=prep_config["modernize"],
            lowercase=prep_config["lowercase"],
            strip_numbers=prep_config["numbers"],
            stopwords=prep_config["stopwords"],
            pos_to_keep=prep_config["pos_to_keep"],
            ascii=prep_config["ascii"],
            min_word_length=prep_config["minimum_word_length"],
            is_philo_db=True,
            workers=args.workers,
            progress=False,
        )
        if metadata_filters:
            file_list, philo_ids = get_file_list(os.path.join(db_data["db_path"], "data"), metadata_filters)
            file_count = len(philo_ids)
        else:
            file_list = [f.path for f in os.scandir(os.path.join(db_data["db_path"], "data/words_and_philo_ids"))]
            file_count = len(file_list)
        metadata = {}

        os.system(f"mkdir -p {os.path.join(training_texts_path, db_name, 'texts')}")
        for text in preproc.process_texts(
            file_list,
            progress_prefix=f"Processing {file_count} files from collection {count} of {len(training_data)}...",
        ):
            if metadata_filters and text.metadata["philo_id"] not in philo_ids:
                continue
            with open(
                os.path.join(training_texts_path, db_name, "texts", str(pos)), "w", 65536
            ) as output:  ## Set buffer to 64K to speed up writes and avoid build-up in RAM
                output.write(" ".join(text))
            if (
                db_name in inference_data
                and db_data["text_object_level"] == inference_data[db_name]["text_object_level"]
            ):  # if training collection and inference collection are the same, we won't process it again
                text.metadata["db_name"] = db_name
                metadata[pos] = text.metadata
            pos += 1
        with open(os.path.join(training_texts_path, db_name, "metadata.json"), "w") as output_metadata:
            json.dump(metadata, output_metadata)

    pos = 0
    for db_name, db_data in inference_data.items():
        count += 1
        if db_name in inference_data:
            if db_data["text_object_level"] == inference_data[db_name]["text_object_level"]:
                os.system(f"ln -s {os.path.abspath(training_texts_path)}/{db_name} {inference_texts_path}/{db_name}")
                continue
        preproc = PreProcessor(
            text_object_type=db_data["text_object_level"],
            language=prep_config["language"],
            stemmer=prep_config["stemmer"],
            lemmatizer=prep_config["lemmatizer"],
            modernize=prep_config["modernize"],
            lowercase=prep_config["lowercase"],
            strip_numbers=prep_config["numbers"],
            stopwords=prep_config["stopwords"],
            pos_to_keep=prep_config["pos_to_keep"],
            ascii=prep_config["ascii"],
            min_word_length=prep_config["minimum_word_length"],
            is_philo_db=True,
            workers=args.workers,
            progress=False,
        )
        if metadata_filters:
            file_list, philo_ids = get_file_list(os.path.join(db_data["db_path"], "data"), metadata_filters)
            file_count = len(philo_ids)
        else:
            file_list = [f.path for f in os.scandir(os.path.join(db_data["db_path"], "data/words_and_philo_ids"))]
            file_count = len(file_list)
        metadata = {}
        os.system(f"mkdir -p {os.path.join(inference_texts_path, db_name, 'texts')}")
        for text in preproc.process_texts(
            file_list,
            progress_prefix=f"Processing {file_count} files from collection {count} of {len(inference_data)}...",
        ):
            if metadata_filters and text.metadata["philo_id"] not in philo_ids:
                continue
            with open(
                os.path.join(inference_texts_path, db_name, "texts", str(pos)), "w", 65536
            ) as output:  ## Set buffer to 64K to speed up writes and avoid build-up in RAM
                output.write(" ".join(text))
            text.metadata["db_name"] = db_name
            metadata[pos] = text.metadata
            pos += 1
        with open(os.path.join(inference_texts_path, db_name, "metadata.json"), "w") as output_metadata:
            json.dump(metadata, output_metadata)


def build_model(
    training_texts_path,
    inference_texts_path,
    training_data,
    inference_data,
    algorithm="lda",
    number_of_topics=100,
    vectorization="tf",
    max_freq=0.9,
    min_freq=0.1,
    max_features=None,
    ngram=2,
    min_tokens_per_doc=0,
    evaluate=False,
):

    # Load and prepare a corpus
    print("Vectorize documents...", flush=True)
    training_corpus = Corpus(
        training_texts_path,
        vectorization=vectorization,
        max_relative_frequency=max_freq,
        min_absolute_frequency=min_freq,
        ngram=ngram,
        min_tokens_per_doc=min_tokens_per_doc,
        max_features=max_features,
    )
    print("training corpus size:", training_corpus.size)
    print("vocabulary size:", len(training_corpus.vectorizer.vocabulary_))

    identical_corpus = True
    if len(training_data) != len(inference_data):
        identical_corpus = False
    if identical_corpus is True:
        for db, db_data in training_data.items():
            if db not in inference_data:
                identical_corpus = False
                break
            if db_data["text_object_level"] != inference_data[db]["text_object_level"]:
                identical_corpus = False
                break

    if identical_corpus is True:
        if min_tokens_per_doc == 0 or evaluate is True:
            full_corpus = training_corpus
        else:
            full_corpus = Corpus(
                training_texts_path,
                vectorizer=training_corpus.vectorizer,
                max_relative_frequency=training_corpus._max_relative_frequency,
                min_absolute_frequency=training_corpus._min_absolute_frequency,
                ngram=training_corpus.ngram,
            )
    else:
        full_corpus = Corpus(
            inference_texts_path,
            vectorizer=training_corpus.vectorizer,
            max_relative_frequency=training_corpus._max_relative_frequency,
            min_absolute_frequency=training_corpus._min_absolute_frequency,
            ngram=training_corpus.ngram,
        )

    # Instantiate a topic model
    if algorithm == "nmf":
        topic_model = NonNegativeMatrixFactorization(training_corpus)
    else:
        topic_model = LatentDirichletAllocation(training_corpus)

    if evaluate is False:
        # Infer topics
        print("Inferring topics...", flush=True)
        topic_model.infer_topics(num_topics=number_of_topics)
        topic_model.infer_and_replace(full_corpus)

    return topic_model, full_corpus, training_corpus


def build_web_app(
    config_path,
    database_name,
    doc_metadata,
    topic_model,
    full_corpus,
    source_file_path,
    source_database_link,
    object_level,
    topics_over_time,
    algorithm="lda",
    number_of_topics=100,
    vectorization="tf",
    max_freq=0.9,
    min_freq=0.1,
    max_features=None,
    ngram=2,
    min_tokens_per_doc=0,
):
    db_path = os.path.join(GLOBAL_CONFIG["WEB_APP"]["web_app_path"], database_name)
    if os.path.exists(db_path) is True:
        os.system(f"rm -rf {db_path}")
    os.mkdir(db_path)
    os.system(f"cp -R /var/lib/topologic/web-app/browser-app/* {db_path}/")
    os.system(f"cp /var/lib/topologic/web-app/apache_htaccess.conf {db_path}/.htaccess")
    config = configparser.ConfigParser()
    config.read(config_path)

    years = set()
    metadata_field_names = set()
    for fields in doc_metadata.values():
        for field in fields.keys():
            metadata_field_names.add(field)
        try:
            years.add(int(fields["year"]))
        except ValueError:
            pass
    if topics_over_time["start_date"] is None:
        min_year = min(years)
    else:
        min_year = topics_over_time["start_date"]
    if topics_over_time["end_date"] is None:
        max_year = max(years)
    else:
        max_year = topics_over_time["end_date"]
    if topics_over_time["topics_over_time_interval"] != 1:
        min_year = year_normalizer(min_year, topics_over_time["topics_over_time_interval"])
        max_year = max_year_normalizer(max_year, topics_over_time["topics_over_time_interval"])

    config["DATA"] = {
        "num_docs": full_corpus.size,
        "num_tokens": len(full_corpus.vectorizer.vocabulary_),
        "metadata": ",".join(metadata_field_names),
        "file_path": source_file_path,
    }

    with open(os.path.join(db_path, "model_config.ini"), "w") as configfile:
        config.write(configfile)

    db = DBHandler.set_class_attributes(
        GLOBAL_CONFIG["DATABASE"],
        database_name,
        topic_model,
        full_corpus,
        doc_metadata,
        min_year,
        max_year,
        topics_over_time["topics_over_time_interval"],
    )
    print("Saving words...", flush=True)
    db.save_words()

    print("Saving docs...", flush=True)
    db.save_docs()

    print("Saving topics...", flush=True)
    db.save_topics(
        f"{db_path}/topic_words.json", min_year, max_year, topics_over_time["topics_over_time_interval"],
    )

    write_app_config(
        source_database_link,
        db_path,
        database_name,
        GLOBAL_CONFIG["WEB_APP"]["server_name"],
        min_year,
        max_year,
        topics_over_time["topics_over_time_interval"],
    )
    os.system(f"cd {db_path}; npm install; npm run build")

    print(
        f"TopoLogic web application is viewable at: {os.path.join(GLOBAL_CONFIG['WEB_APP']['server_name'], 'topologic', os.path.basename(db_path))}"
    )


if __name__ == "__main__":
    args = parse_args()
    main(args)
