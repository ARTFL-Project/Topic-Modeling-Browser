#!/usr/bin/env python3

import argparse
import configparser
import json
import os

from text_preprocessing import PreProcessor
from topic_modeling_browser import read_config, write_app_config
from topic_modeling_browser.corpus import Corpus
from topic_modeling_browser.DB import DBHandler
from topic_modeling_browser.topic_model import LatentDirichletAllocation, NonNegativeMatrixFactorization
from topic_modeling_browser import topic_num_evaluator
from joblib import dump


GLOBAL_CONFIG = configparser.ConfigParser()
GLOBAL_CONFIG.read("/etc/topic-modeling-browser/global_settings.ini")


def parse_args():
    parser = argparse.ArgumentParser(description="Define files to process")
    parser.add_argument("--config", help="Configuration file", default="", type=str)
    parser.add_argument(
        "--data_output", help="path to local data to be saved during processing", default="/tmp", type=str
    )
    parser.add_argument(
        "--workers", help="How many threads or cores to use for preprocessing and modeling", type=int, default=4
    )
    parser.add_argument(
        "--evaluate",
        help="Evaluate topic model. No topic models or web app will be saved",
        action="store_true",
        default=False,
    )
    parser.add_argument("--min_num_topics", help="minimum number of topics for evaluation", type=int, default=10)
    parser.add_argument("--max_num_topics", help="maximum number of topics for evaluation", type=int, default=20)
    parser.add_argument(
        "--debug", help="debug mode: temp file in /tmp will not be deleted.", action="store_true", default=False
    )
    args = parser.parse_args()
    return args


def main(args):
    if args.config == "":
        print("No configuration file provided, exiting...")
        exit()
    source_data, database_name, prep_config, vector_config, model_config = read_config(args.config)
    if args.data_output == "/tmp":
        args.data_output = os.path.join(args.data_output, database_name)
    if os.path.exists(args.data_output) is True:
        os.system(f"rm -rf {args.data_output}")
    texts_path = os.path.join(args.data_output, "texts")
    os.system(f"mkdir -p {texts_path}")
    metadata = {}
    source_data_path = os.path.join(source_data["philologic_database_path"], "data/words_and_philo_ids")

    print("## PREPROCESSING ##", flush=True)
    print(f"Processing {sum(1 for _ in os.scandir(source_data_path))} files...", flush=True)
    preproc = PreProcessor(
        text_object_type=prep_config["text_object_level"],
        language=prep_config["language"],
        stemmer=prep_config["stemmer"],
        lemmatizer=prep_config["lemmatizer"],
        modernize=prep_config["modernize"],
        lowercase=prep_config["lowercase"],
        strip_numbers=prep_config["numbers"],
        stopwords=prep_config["stopwords"],
        pos_to_keep=prep_config["pos_to_keep"],
        ascii=prep_config["ascii"],
        min_word_length=prep_config["minimum_word_length"],
        is_philo_db=True,
        workers=args.workers,
        progress=False,
    )
    for pos, text in enumerate(preproc.process_texts((f.path for f in os.scandir(source_data_path)))):
        with open(
            os.path.join(texts_path, str(pos)), "w", 65536
        ) as output:  ## Set buffer to 64K to speed up writes and avoid build-up in RAM
            output.write(" ".join(text))
        metadata[pos] = text.metadata

    source_file_path = os.path.join(source_data["philologic_database_path"], "data/TEXT")
    topic_model, full_corpus, partial_corpus = build_model(
        texts_path,
        metadata,
        algorithm=model_config["algorithm"],
        number_of_topics=model_config["number_of_topics"],
        vectorization=vector_config["vectorization"],
        max_freq=vector_config["max_freq"],
        min_freq=vector_config["min_freq"],
        max_features=vector_config["max_features"] or None,
        ngram=vector_config["ngram"],
        min_tokens_per_doc=vector_config["min_tokens_per_doc"],
        evaluate=args.evaluate,
    )

    if args.evaluate is False:
        build_web_app(
            database_name,
            metadata,
            topic_model,
            full_corpus,
            source_file_path,
            source_data["philologic_database_url"],
            algorithm=model_config["algorithm"],
            number_of_topics=model_config["number_of_topics"],
            vectorization=vector_config["vectorization"],
            max_freq=vector_config["max_freq"],
            min_freq=vector_config["min_freq"],
            max_features=vector_config["max_features"] or None,
            ngram=vector_config["ngram"],
            min_tokens_per_doc=vector_config["min_tokens_per_doc"],
        )
    else:
        print("Estimating the number of topics...")
        corpus_path = os.path.join(args.data_output, "corpus")
        dump(partial_corpus, corpus_path)
        os.system("mkdir -p ./evaluation_output")
        topic_num_evaluator(
            corpus_path,
            args.min_num_topics,
            args.max_num_topics,
            model_config["algorithm"],
            iterations=10,
            step=1,
            top_n_words=10,
            workers=args.workers,
        )

    if args.debug is False:
        os.system(f"rm -rf {args.data_output}")


def build_model(
    texts_path,
    doc_metadata,
    algorithm="lda",
    number_of_topics=100,
    vectorization="tf",
    max_freq=0.9,
    min_freq=0.1,
    max_features=None,
    ngram=2,
    min_tokens_per_doc=0,
    evaluate=False,
):

    # Load and prepare a corpus
    print("Vectorize documents...", flush=True)
    partial_corpus = Corpus(
        texts_path,
        doc_metadata,
        vectorization=vectorization,
        max_relative_frequency=max_freq,
        min_absolute_frequency=min_freq,
        ngram=ngram,
        min_tokens_per_doc=min_tokens_per_doc,
        max_features=max_features,
    )
    print("corpus size:", partial_corpus.size)
    print("vocabulary size:", len(partial_corpus.vectorizer.vocabulary_))

    if min_tokens_per_doc == 0 or evaluate is True:
        full_corpus = partial_corpus
    else:
        full_corpus = Corpus(
            texts_path,
            doc_metadata,
            vectorizer=partial_corpus.vectorizer,
            max_relative_frequency=partial_corpus._max_relative_frequency,
            min_absolute_frequency=partial_corpus._min_absolute_frequency,
            ngram=partial_corpus.ngram,
        )

    # Instantiate a topic model
    if algorithm == "nmf":
        topic_model = NonNegativeMatrixFactorization(partial_corpus)
    else:
        topic_model = LatentDirichletAllocation(partial_corpus)

    if evaluate is False:
        # Infer topics
        print("Inferring topics...", flush=True)
        topic_model.infer_topics(num_topics=number_of_topics)
        topic_model.infer_and_replace(full_corpus)

    return topic_model, full_corpus, partial_corpus


def build_web_app(
    database_name,
    doc_metadata,
    topic_model,
    full_corpus,
    source_file_path,
    source_database_link,
    algorithm="lda",
    number_of_topics=100,
    vectorization="tf",
    max_freq=0.9,
    min_freq=0.1,
    max_features=None,
    ngram=2,
    min_tokens_per_doc=0,
):
    db_path = os.path.join(GLOBAL_CONFIG["WEB_APP"]["web_app_path"], database_name)
    if os.path.exists(db_path) is True:
        os.system(f"rm -rf {db_path}")
    os.mkdir(db_path)
    os.system(f"cp -R /var/lib/topic-modeling-browser/web-app/browser-app/* {db_path}/")
    os.system(f"cp /var/lib/topic-modeling-browser/web-app/apache_htaccess.conf {db_path}/.htaccess")
    config = configparser.ConfigParser()
    config["PARAMETERS"] = {
        "number_of_topics": number_of_topics,
        "algorithm": algorithm,
        "vectorization": vectorization,
        "max_freq": max_freq,
        "min_freq": min_freq,
        "ngram": ngram,
    }

    years = set()
    metadata_field_names = set()
    for fields in doc_metadata.values():
        for field in fields.keys():
            metadata_field_names.add(field)
        try:
            years.add(int(fields["year"]))
        except ValueError:
            pass

    config["DATA"] = {
        "num_docs": full_corpus.size,
        "num_tokens": len(full_corpus.vectorizer.vocabulary_),
        "metadata": ",".join(metadata_field_names),
        "file_path": source_file_path,
    }

    with open(os.path.join(db_path, "model_config.ini"), "w") as configfile:
        config.write(configfile)

    db = DBHandler.set_class_attributes(
        GLOBAL_CONFIG["DATABASE"], database_name, topic_model, full_corpus, doc_metadata
    )
    print("Saving words...", flush=True)
    db.save_words()

    print("Saving docs...", flush=True)
    db.save_docs()

    print("Saving topics...", flush=True)
    db.save_topics(f"{db_path}/topic_words.json", min(years), max(years), step=1)

    write_app_config(
        source_database_link, db_path, database_name, GLOBAL_CONFIG["WEB_APP"]["server_name"], min(years), max(years)
    )
    os.system(f"cd {db_path}; npm install; npm run build")


if __name__ == "__main__":
    args = parse_args()
    main(args)
