#!/usr/bin/env python3

import argparse
import configparser
import json
import os
import re
import shutil
import sys

import dill as pickle
from tqdm import tqdm
from topic_modeling_browser.topic_model import LatentDirichletAllocation, NonNegativeMatrixFactorization
from topic_modeling_browser.corpus import Corpus
from topic_modeling_browser.DB import DBHandler
from text_preprocessing import PreProcessor

TAGS = re.compile(r"<[^>]+>")
START_TAG = re.compile(r"^[^<]*?>")

GLOBAL_CONFIG = configparser.ConfigParser()
GLOBAL_CONFIG.read("/etc/topic-modeling-browser/global_settings.ini")


def parse_args():
    parser = argparse.ArgumentParser(description="Define files to process")
    parser.add_argument("--corpus", dest="corpus", type=str, default="")
    parser.add_argument("--config", help="Configuration file", default="", type=str)
    parser.add_argument(
        "--is_philo_db", help="Define if input files are preprocessed by PhiloLogic", action="store_true", default=False
    )
    parser.add_argument(
        "--data_output", help="path to local data to be saved during processing", default="/tmp", type=str
    )
    parser.add_argument(
        "--workers", help="How many threads or cores to use for preprocessing and modeling", type=int, default=4
    )
    args = parser.parse_args()
    return args


def read_config(config_path):
    config = configparser.ConfigParser()
    config.read(config_path)
    preprocessing = {}
    for key, value in config["PREPROCESSING"].items():
        if key == "pos_to_keep" and value != "":
            preprocessing[key] = [i.strip() for i in value.split(",")]
        else:
            preprocessing[key] = value
    vectorization = {}
    for key, value in config["VECTORIZATION"].items():
        if key in ("min_freq", "max_freq"):
            vectorization[key] = float(value.strip())
        elif key in ("ngram", "min_tokens_per_doc"):
            vectorization[key] = int(value.strip())
        elif key == "max_features":
            if value:
                vectorization[key] = int(value.strip())
            else:
                vectorization[key] = None
        else:
            vectorization[key] = value
    topic_modeling = {}
    for key, value in config["TOPIC_MODELING"].items():
        if key in ("number_of_topics", "max_iter"):
            topic_modeling[key] = int(value.strip())
        else:
            topic_modeling[key] = value
    return config["DATABASE"]["database_name"], preprocessing, vectorization, topic_modeling


def write_app_config(db_path, database_name):
    with open(os.path.join(db_path, "appConfig.json"), "w") as app_config:
        json.dump(
            {
                "webServer": "Apache",
                "apiServer": os.path.join(GLOBAL_CONFIG["WEB_APP"]["server_name"], "topic-modeling-browser-api"),
                "databaseName": database_name,
                "appPath": os.path.join("topic-modeling-browser", database_name),
            },
            app_config,
        )


def main(args):
    if args.config == "":
        print("No configuration file provided, exiting...")
        exit()
    database_name, prep_config, vector_config, model_config = read_config(args.config)
    if args.data_output == "/tmp":
        args.data_output = os.path.join(args.data_output, database_name)
    print(f"Saving temporary files to {args.data_output}...", flush=True)
    if os.path.exists(args.data_output) is True:
        os.system(f"rm -rf {args.data_output}")

    preproc = PreProcessor(
        text_object_type=prep_config["text_object_level"],
        language=prep_config["language"],
        stemmer=prep_config["stemmer"],
        lemmatizer=prep_config["lemmatizer"],
        modernize=prep_config["modernize"],
        lowercase=prep_config["lowercase"],
        strip_numbers=prep_config["numbers"],
        stopwords=prep_config["stopwords"],
        pos_to_keep=prep_config["pos_to_keep"],
        ascii=prep_config["ascii"],
        is_philo_db=args.is_philo_db,
        workers=args.workers,
        progress=False,
    )

    texts_path = os.path.join(args.data_output, "texts")
    os.system(f"mkdir -p {texts_path}")

    metadata = {}
    for pos, text in enumerate(preproc.process_texts((f.path for f in os.scandir(args.corpus)))):
        with open(os.path.join(texts_path, str(pos)), "w") as output:
            output.write(" ".join(text))
        metadata[pos] = text.metadata

    if args.is_philo_db is True:
        source_file_path = os.path.join(os.path.dirname(args.corpus.rstrip("/")), "TEXT")
    else:
        source_file_path = args.corpus

    build_model(
        texts_path,
        metadata,
        source_file_path,
        database_name,
        algorithm=model_config["algorithm"],
        number_of_topics=model_config["number_of_topics"],
        vectorization=vector_config["vectorization"],
        max_freq=vector_config["max_freq"],
        min_freq=vector_config["min_freq"],
        max_features=vector_config["max_features"] or None,
        n_gram=vector_config["ngram"],
        min_tokens_per_doc=vector_config["min_tokens_per_doc"],
    )


def build_model(
    texts_path,
    doc_metadata,
    source_file_path,
    database_name,
    algorithm="lda",
    number_of_topics=100,
    vectorization="tf",
    max_freq=0.9,
    min_freq=0.1,
    max_features=None,
    n_gram=2,
    min_tokens_per_doc=0,
):

    # Load and prepare a corpus
    print("Vectorize documents...", flush=True)
    partial_corpus = Corpus(
        texts_path,
        doc_metadata,
        vectorization=vectorization,
        max_relative_frequency=max_freq,
        min_absolute_frequency=min_freq,
        n_gram=n_gram,
        min_tokens_per_doc=min_tokens_per_doc,
        max_features=max_features,
    )
    print("corpus size:", partial_corpus.size)
    print("vocabulary size:", len(partial_corpus.vectorizer.vocabulary_))

    if min_tokens_per_doc == 0:
        full_corpus = partial_corpus
    else:
        full_corpus = Corpus(
            texts_path,
            doc_metadata,
            vectorizer=partial_corpus.vectorizer,
            max_relative_frequency=partial_corpus._max_relative_frequency,
            min_absolute_frequency=partial_corpus._min_absolute_frequency,
            n_gram=partial_corpus._n_gram,
        )

    db_path = os.path.join(GLOBAL_CONFIG["WEB_APP"]["web_app_path"], database_name)
    if os.path.exists(db_path) is True:
        os.system(f"rm -rf {db_path}")
    os.mkdir(db_path)
    os.system(f"cp -R /var/lib/topic-modeling-browser/web-app/browser-app/* {db_path}/")
    os.system(f"cp /var/lib/topic-modeling-browser/web-app/apache_htaccess.conf {db_path}/.htaccess")
    config = configparser.ConfigParser()
    config["PARAMETERS"] = {
        "number_of_topics": number_of_topics,
        "algorithm": algorithm,
        "vectorization": vectorization,
        "max_freq": max_freq,
        "min_freq": min_freq,
    }

    years = set()
    metadata_field_names = set()
    for fields in doc_metadata.values():
        for field in fields.keys():
            metadata_field_names.add(field)
        try:
            years.add(int(fields["year"]))
        except ValueError:
            pass

    config["DATA"] = {
        "num_docs": full_corpus.size,
        "num_tokens": len(full_corpus.vectorizer.vocabulary_),
        "metadata": ",".join(metadata_field_names),
        "file_path": source_file_path,
    }

    with open(os.path.join(db_path, "model_config.ini"), "w") as configfile:
        config.write(configfile)

    # Instantiate a topic model
    if algorithm == "nmf":
        topic_model = NonNegativeMatrixFactorization(partial_corpus)
    else:
        topic_model = LatentDirichletAllocation(partial_corpus)

    # Infer topics
    print("Inferring topics...", flush=True)
    topic_model.infer_topics(num_topics=number_of_topics)
    topic_model.infer_and_replace(full_corpus)

    print("Compute document similarity...", flush=True)
    full_corpus.similar_documents_by_topic_distribution(topic_model)

    db = DBHandler(GLOBAL_CONFIG["DATABASE"], database_name)
    print("Saving words...", flush=True)
    db.save_words(topic_model, full_corpus)

    print("Saving docs...", flush=True)
    db.save_docs(topic_model, full_corpus, doc_metadata)

    print("Saving topics...", flush=True)
    db.save_topics(
        f"{db_path}/topic_words.json", topic_model, full_corpus, min(years), max(years), doc_metadata, step=1
    )

    write_app_config(db_path, database_name)
    os.system(f"cd {db_path}; npm install; npm run build")


if __name__ == "__main__":
    args = parse_args()
    main(args)
